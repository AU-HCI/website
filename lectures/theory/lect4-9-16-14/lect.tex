\documentclass{article}
\usepackage{parskip}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{color}
\usepackage{graphicx}
\usepackage{mathpartir}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{definition}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{notn}[thm]{Notation}
\newtheorem{observation}[thm]{Observation}

\newcommand {\redto}[0]{\rightsquigarrow}

\input{lam-ott}

\title{Computing in the $\lambda$-Calculus\\Programming Languages (CSCI 3300), Fall 2014\vspace{-22px}}
\author{Prof. Harley Eades (heades@gru.edu).}
\date{\vspace{-22px}}
\begin{document}
\maketitle  

\section{Inference Systems}
\label{sec:inference_rules}

A lot of the design of programming languages in general involve the
careful definition of certain algorithms that manipulate programs.
This is similar to the idea of defining functions on programs like the
$\mathsf{depth}$ function we saw in class.  A large number of
algorithms can be described using inference systems.

An inference system consists of one or more inference rules.  An
inference rule has the following shape:
\begin{center}
  \begin{math}
    $$\mprset{flushleft}
    \inferrule* [right=Name] {
      P_1
      \\
      \cdots
      \\
      P_n
    }{C}
  \end{math}
\end{center}
We call $P_1, \cdots, P_n$ the premises of the rule, and $C$ the
conclusion.  Note that it is completely satisfactory to have an
inference rule with no premises at all. In fact, an inference rules
with no premises are called axioms. Inference rules are read from the
top down as an if-statement.  So the above rule should be read "If
$P_1$, $P_2$, $\ldots$, and $P_n$ all hold, then $C$ holds."  If an
inference rule has no premises at all, then its conclusion always
holds, hence, the term axiom.  

An example may help.  The following set of inference rules define what
it means to be a binary tree:
\begin{center}
  \begin{math}
    \begin{array}{cc}
      $$\mprset{flushleft}
      \inferrule* [right=leaf] {
        n \in \mathbb{N}
      }{(\mathsf{leaf}\,n)\,Tree}
      &
      $$\mprset{flushleft}
      \inferrule* [right=node] {
        T_1\,Tree
        \\
        T_2\,Tree
        \\
        n \in \mathbb{N}
      }{(\mathsf{node}\,n\,T_1\,T_2)\,Tree}
    \end{array}
  \end{math}
\end{center}
The first rule states, "for all $n \in \mathbb{N}$, $\mathsf{leaf}\,n$
is a tree." Now the second states that "for any $T_1$, $T_2$, and $n
\in \mathbb{N}$, if $T_1$ and $T_2$ are trees, then
$\mathsf{node}\,n\,T_1\,T_2$ is a tree."

So what are these rules good for?  Inference system should be thought
of as an algorithm where the conclusion, $C$, is an input, and the
output essentially amounts to yes or no.  The former indicating that
the input adheres to the rules, and no otherwise.  So for example,
given the rules above we can ask is
$\mathsf{node}\,1\,(\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\,
(\mathsf{leaf}\, 5))\, (\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,
(\mathsf{leaf}\, 7))$ a tree, but how can we actually answer this
question?  We answer this question by building derivations of the
rules.  This is all very similar to how we used grammars.

A derivation is the application of one or more rules.  For example,
one of the simplest derivations using the rules for trees is the
following:
\begin{center}
  \begin{math}
    $$\mprset{flushleft}
    \inferrule* [right=leaf] {
      1 \in \mathbb{N}
    }{(\mathsf{leaf}\,1)\,Tree}
  \end{math}
\end{center}
In the above $\mathsf{leaf}\,1$ is the input, and the derivation
consists of only the rule $\mathtt{leaf}$, and this successfully shows
that $\mathsf{leaf}\,1$ is indeed a tree.  A more complete derivation
is the follows:
  \begin{center}
    \scriptsize
    \begin{math}
      $$\mprset{flushleft}
    \inferrule* [right=\scriptsize node] {
      $$\mprset{flushleft}
      \inferrule* [right=\scriptsize node] {
        $$\mprset{flushleft}
        \inferrule* [right=\scriptsize leaf] {
          4 \in \mathbb{N}
        }{(\mathsf{leaf}\, 4)\,Tree}
        \\
        $$\mprset{flushleft}
        \inferrule* [right=\scriptsize leaf] {
          5 \in \mathbb{N}
        }{(\mathsf{leaf}\,5)\,Tree}
        \\
        2 \in \mathbb{N}
      }{(\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\,(\mathsf{leaf}\, 5))\,Tree}
      \\
      $$\mprset{flushleft}
      \inferrule* [right=\scriptsize node] {                
        $$\mprset{flushleft}
        \inferrule* [right=\scriptsize leaf] {
          6 \in \mathbb{N}
        }{(\mathsf{leaf}\, 6)\,Tree}
        \\
        $$\mprset{flushleft}
        \inferrule* [right=\scriptsize leaf] {
          7 \in \mathbb{N}
        }{(\mathsf{leaf}\, 7)\,Tree}
        \\
        3 \in \mathbb{N}
      }{(\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,(\mathsf{leaf}\, 7))\,Tree}
      \\
      1 \in \mathbb{N}
    }{(\mathsf{node}\,1\,(\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\, (\mathsf{leaf}\, 5))\, (\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,(\mathsf{leaf}\, 7)))\,Tree}
    \end{math}
  \end{center}
  Derivations are constructed from the bottom up.  In the above we
  begin by asking the question, ``does
  $(\mathsf{node}\,1\,(\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\,
  (\mathsf{leaf}\, 5))\, (\mathsf{node}\, 3\, (\mathsf{leaf}\,
  6)\,(\mathsf{leaf}\, 7)))\,Tree$ hold?''  To prove that it holds we
  try to apply the rules of the tree inference system one at a time,
  from the bottom up.  So after asking the question we try and pattern
  match on the conclusion of each rule.  If what we are asking matches
  the conclusion of the rule, then we may then move to the premises of
  that rule.  For example, we construct the above derivation by
  starting like this, we first asking the question:
  \begin{center}
    \begin{math}
      (\mathsf{node}\,1\,(\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\, (\mathsf{leaf}\, 5))\, (\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,(\mathsf{leaf}\, 7)))\,Tree
    \end{math}
  \end{center}
  Then we try to pattern match on the previous statement against the
  conclusion of each rule, and we can see that it matches the
  conclusion of the $\mathtt{node}$ rule, where $n = 1$, $T_1 =
  (\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\, (\mathsf{leaf}\, 5))$, and
  $T_2 = (\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,(\mathsf{leaf}\,
  7))$.  So we are allowed to apply that rule:
    \begin{center}
    \scriptsize
    \begin{math}
      $$\mprset{flushleft}
      \inferrule* [right=\scriptsize node] {
        (\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\,(\mathsf{leaf}\, 5))\,Tree
        \\
        (\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,(\mathsf{leaf}\, 7))\,Tree
        \\
        1 \in \mathbb{N}
      }{(\mathsf{node}\,1\,(\mathsf{node}\,2\, (\mathsf{leaf}\, 4)\, (\mathsf{leaf}\, 5))\, (\mathsf{node}\, 3\, (\mathsf{leaf}\, 6)\,(\mathsf{leaf}\, 7)))\,Tree}
    \end{math}
  \end{center}
  Then we ask, which rule can be applied to each premise?  Then we
  proceed until no further rules can be applied.  Eventually we end up
  with the derivation from above.

  Now how do we know when a derivation is successful?  First, notice
  that the derivations are themselves tree structures. The axioms are
  the leaves of the tree, and the premises start branches of the tree.
  For example, in the above example of the complete derivation using
  the tree inference system we can see that the $\mathtt{node}$ rule
  forms two branches, one for each subtree.  Now a derivation is
  \textbf{successful} if and only if every branch of the derivation
  ends in an axiom.  That is, the very last rules on the top of the
  derivation must be all axioms.  If not, then the derivation is a
  failure, and the conclusion is said not to hold.  The following is
  an example of a failure:
  \begin{center}
    \begin{math}
      $$\mprset{flushleft}
      \inferrule* [right=node] {
        $$\mprset{flushleft}
        \inferrule* [right=leaf] {
          2 \in \mathbb{N}
        }{(\mathsf{leaf}\,2)\,Tree}
        \\
        42
        \\
        1 \in \mathbb{N}
      }{(\mathsf{node}\,1\,(\mathsf{leaf}\,2)\,42)\,Tree}
    \end{math}
  \end{center}
  The previous derivation fails, because there is no rule to apply
  $42$ to and thus the derivation does not end in all axioms -- that
  is the $\mathtt{leaf}$ rule.  Therefore, we must conclude that
  $\mathsf{node}\,1\,(\mathsf{leaf}\,2)\,42$ is not a valid tree.  

  Inference systems usually have a particular goal called a judgment.
  The inference system is said to judge whether not something is
  provable or not.  For example, the tree inference system judges
  whether or not some mathematical object is a tree, and we can read
  the judgement $t\,Tree$ as ``$t$ is a tree.''  We then say that a
  judgment holds or is provable or is derivable if and only if we can
  construct a successful derivation of it.

  Inference systems are everywhere in programming language design.
  They allow for a very nice, compact, and easy definitions of
  algorithms on the objects of a programming language. Now we can use
  inference systems to define how we can compute in the
  $\lambda$-calculus.
% section inference_rules (end)

\section{Computing in the $\lambda$-Calculus}
\label{sec:computing_in_the_lambda-calculus}

The locus of computation in the $\lambda$-calculus is function
application, $[[t1 t2]]$, but even more specifically it is the
application of a $\lambda$-abstraction to an argument, that is,
$[[(\x.t) t']]$.  It turns out that computation in the
$\lambda$-calculus is really just a matter of replacing every
occurrence of $[[x]]$ in $[[t]]$ with $[[t']]$, but how do we do this
formally?  We use an inference system:
\begin{center}
  \begin{math}
    \begin{array}{cc}
      \ottdruleBeta{} &      \ottdruleLam{}  \\
      & \\
      \ottdruleAppOne{} &       \ottdruleAppTwo{}\\
    \end{array}
  \end{math}
\end{center}
The judgment here is $[[t1]] \rightsquigarrow [[t2]]$ which can be
read as $[[t1]]$ reduces to $[[t2]]$.  We call this set of inference
rules the reduction rules for the $\lambda$-calculus.  Note that one
should think of the terms in these rules as schemas of terms that we
match pattern against.  So some care has to be taken when checking to
see if a rule applies.  The reduction rules should be used just as we
used the tree inference system above.  That is, we begin with a
judgment that looks like $[[t1 ~> t2]]$, and then try to prove that
$[[t1]]$ reduces to $[[t2]]$ if we can construct a successful
derivation of $[[t1 ~> t2]]$ using the reduction rules.  We can think
of this style as starting with some solution to a problem, and then
using the rules to check to see if the solution is correct.

The most important rule is the $\beta$-rule:
\[\ottdruleBeta{} \]
This rule says that any term matching the pattern $[[(\x.t) t']]$ can
be replaced by $[t'/x]t$ which replaces every occurrence of $[[x]]$
with $[[t']]$ in $[[t]]$. 
\vspace{10px}
\begin{definition}[Redex]
  \label{def:redex}
  Any term matching $[[(\x.t) t']]$ is called a \textbf{redex}.
\end{definition}
So we can see that the term in the lefthand side of the arrow in the
$\beta$-rule is the redex, and we call the righthand side the
\textbf{contractum}.  Notice that variable substitution is an
important part of the $\beta$-rule, and now we can see that computing
really amounts to using the $\beta$-rule which amounts to just using
the variable substitution function to simplify function applications.

Now lets consider an example using the $\beta$-rules.  The following shows
that $[[(\x.x) y]]$ reduces to $y$.
\begin{center}
  \begin{math}
    $$\mprset{flushleft}
    \inferrule* [right=Beta] {
      \,
    }{[[(\x.x) y ~> y]]}
  \end{math}
\end{center}
This holds because the term $[[(\x.x) y]]$ matches the pattern
$[[(\x.t) t']]$ where the binder $[[x]]$ is $[[x]]$, $[[t]]$ is
$[[x]]$, and $[[t']]$ is $[[t]]$, and $[[ [y/x]x ]] = [[y]]$.  So
sometimes this rule is hard to apply because we have to reverse the
substitution function, that is, we had to realize the $[[y]]$ is the
same as $[[ [y/x]x]]$ when just given $[[y]]$.

The reduction rules are literal.  One can only apply the rule if the
terms given match the term patterns in the conclusion.  For example,
can we apply the $\beta$-rule to conclude $[[x ((\y.h(y z)) r) ~> x (r
z)]]$?  No, because $[[x ((\y.h(y z)) r)]]$ does not match the pattern
$[[(\x.t) t']]$, and $[[x (r z)]]$ does not match the pattern $[[
[t'/x]t]]$ so that rule does not apply, but we should be allowed to
reduce that redex, right?  This is the job for the other reduction
rules.


There are three other reduction rules:
\begin{center}
  \begin{math}
    \begin{array}{lll}
      \ottdruleLam{}  & \ottdruleAppOne{} \\
      & \\
      \ottdruleAppTwo{}\\
    \end{array}
  \end{math}
\end{center}
These rules allow us to move inside of terms.  For example, we can
now show $[[x ((\y.h(y z)) r) ~> x (r z)]]$:
\begin{center}
  \begin{math}
    $$\mprset{flushleft}
    \inferrule* [right=App2] {
      $$\mprset{flushleft}
      \inferrule* [right=Beta] {
        \,
      }{[[(\y.h(y z)) r ~> (r z)]]}
    }{[[x ((\y.h(y z)) r) ~> x (r z)]]}
  \end{math}
\end{center}

Consider this term for example, $[[(\x.h(x x))((\y.y) r)]]$, how many
redexes are there in this term? There are two, the entier term, and
the subterm $[[(\y.y) r]]$.  So how do we reduce them both?  In fact,
we cannot do this, because these rules only support reducing exactly
one redex.  In fact, the reduction rules we have considered so far are
called the \textbf{single-step full $\beta$-reduction.}  The ``full''
part has to do with the ability to contract a redex anywhere within a
term.

To allow for the reduction of multiple redexes we define a new
inference system called \textbf{multi-step full $\beta$-reduction.}
The inference system is as follows:
\begin{center}
  \begin{math}
    \begin{array}{lll}
     \ottdruleRefl{} & \ottdruleSStep{} & \ottdruleMStep{}
    \end{array}
  \end{math}
\end{center}

Now we can use this inference system to reduce more than one redex at a time.
For example we can prove the following reductions (full derivations in class):
\begin{center}
  \begin{math}
    \begin{array}{lll}
      [[h(x ((\y.y)((\z.h(z z)) r))) ~*> x (r r)]] \\
      [[\y.h((\x.\y.y)((\x.h(x x))(\x.h(x x))) z) ~*> \y.z]]
    \end{array}
  \end{math}
\end{center}

There is one downfall of applying the reduction rules the way that we
have.  The redex is often the program the programmer has written, and
the contractum is either the solution, or an intermediate point to the
solution.  So from here on out we -- unless directed differently like
in homeworks or exams -- we are going to adopted a different way of
applying these rules.  We will use these rules a guiding principle for
how to transform a term into another by contracting its redexes.  So
instead of writing down long and big derivations we will simply write
down a chain of transformations.  These transformations a very similar
to the derivations we did on grammars.  Lets consider an example.
\begin{center}
  \begin{math}
    \begin{array}{lll}
      [[ul((\n.h(\m.(\s.h(\z.h((n s) (m s z)))))) (\s.h(\z.h(s z))) (\s.h(\z.h(s (s z)))))]] \\
      \,\,\,\,\,\,\,\,\,\,\rightsquigarrow [[ul((\m.(\s.h(\z.h(((\s.h(\z.h(h(s z) s))) (m s z)))))) (\s.h(\z.h(s (s z)))))]]\\
      \,\,\,\,\,\,\,\,\,\,\rightsquigarrow [[\s.h(\z.h((((\s.h(\z.h(h(s z)))) s) ul(((\s.h(\z.h(s (s z)))) s z)))))]]\\
      \,\,\,\,\,\,\,\,\,\,\rightsquigarrow [[\s.h(\z.h((ul(((\s.h(\z.h(h(s z)))) s)) (s (s z)))))]]\\
      \,\,\,\,\,\,\,\,\,\,\rightsquigarrow [[\s.h(\z.ul(h((\z.h(s z)) (s (s z)))))]]\\
      \,\,\,\,\,\,\,\,\,\,\rightsquigarrow [[\s.h(\z.h(s (s (s z))))]]\\
    \end{array}
  \end{math}
\end{center}
Notice how in the above I underlined the redex I was contracting in
each step.  This is required when writing these down in your homework
and exams, because it makes it apparent which redex we are contracting
at each step.  Also, notice that where we can put the underline is
determined by the rule of the inference system.  Lastly, each
transformation is step in the single-step full $\beta$-reduction
inference system.  However, if one can give a chain of one or more
transformations from $[[t1]]$ to $[[t2]]$, then we can derive $[[t1
~*> t2]]$ in the multi-step full $\beta$-reduction inference system.
% section computing_in_the_lambda-calculus (end)

% \section{Reduction Order}
% \label{sec:reduction_order}
% The reduction rules for the $\lambda$-calculus do not enforce any
% particular order of reduction.  That is, we can choose to contract any
% redex we wish.  This does have an important consequence. Consider the
% following $\lambda$-expression:
% \[ [[h((\x.\y.x) z) ((\x.h(x x)) (\x.h(x x)))]] \] Suppose we choose
% to always reduce the second argument, then the previous expression
% will run forever.  Now suppose that we always choose to reduce the
% left-most redex, then we will obtain the following reduction sequence:
% \begin{center}
%   \begin{math}
%     \begin{array}{lll}
%       [[h((\x.\y.x) z) ((\x.h(x x)) (\x.h(x x)))]] & \redto & [[h(\y.z) ((\x.h(x x)) (\x.h(x x)))]]\\
%       & \redto & [[z]]\\
%     \end{array}
%   \end{math}
% \end{center}
% This example shows that the reduction order has a pretty dramatic
% effect.  It can determine whether or not a program terminates!
% % section reduction_order (end)

% \section{Termination}
% \label{sec:termination}
% The $\lambda$-calculus is the craziest of crazy programming languages.
% You can write terminating programs as well as non-terminating
% programs.  For example, the over used $\lambda$-expression called
% $\Omega$ is defined as $[[(\x.h(x x))(\x.h(x x))]]$.  This clearly
% does not terminate.  In the previous section we saw that depending on
% which redex one chooses to reduce a program may terminate or diverge
% -- never terminate.  Is it possible to give a reduction order -- a
% specific ordering on which redex to choose -- that will guarantee that
% if a program terminates, then it will?

% It turns out that we can, and the reduction order is called
% normal-order reduction.  The rule is that one must \textbf{always}
% reduce the left-most outer-most redex in a $\lambda$-expression.  If
% one does this, and the $\lambda$-expression being reduced can
% terminate, then normal-order reduction will terminate.
% % section termination (end)

% \section{Normal-Order Reduction}
% \label{sec:normal-order_reduction}
% In this section we modify the definition of the $\lambda$-calculus to
% restrict the reduction to normal-order only.   To do this we introduce
% a new concept called a evaluation contexts.  These are simply a
% different presentation of the rules we have now.  

% Before defining normal-order reduction lets first redefine full
% $\beta$-reduction using evaluation contexts.  The syntax of the
% $\lambda$-calculus is redefined as follows:
% \begin{center}
%   \begin{math}
%     \begin{array}{lll}
%       \text{(Terms)}    & [[t]] ::= [[x]] \mid [[\x.t]] \mid [[t1 t2]]\\
%       \text{(Contexts)} & [[C]] ::= [[*]] \mid [[\x.C]] \mid [[t C]] \mid [[C t]]\\
%     \end{array}
%   \end{math}
% \end{center}
% We call $[[*]]$ the hole.  So a context $[[C]]$ is a $\lambda$-term
% with a hole in it.  Notice that we can only have one hole.  This is
% key.  The hole denotes the location of where the redex we are going to
% contact resides.  We denote filling this hole, that is replacing
% $[[*]]$ in a context $[[C]]$ with a term $[[t]]$, by $[[C[t] ]]$. Suppose our context $[[C]]$ is $[[\x.h(z *)]]$, then, $[[C[ \y.h(y y)] ]] = [[(\x.h(z *))[\y.h(y y)] ]] = [[\x.h(z (\y.h(y y)))]]$.  We can see that we simply just replaced (plugged) the
% $[[*]]$ (the hole) with $[[\y.h(y y)]]$.  In addition, the context
% $[[C]]$ encapsulates the term around the hole or where the redex
% resides.  So contexts formally define ``ripping out the redex'' as I
% have been saying during lecture.

% Now using evaluation contexts we can collapse all of our inference
% rules into one rule only:
% \begin{center}
%   \begin{math}
%     $$\mprset{flushleft}
%     \inferrule* [right=Beta] {
%       \,
%     }{[[C[(\x.t) t'] ]] \redto [[C[ [t'/x]t] ]]}
%   \end{math}
% \end{center}
% The congurence rules are encoded into the context, because we can
% contract any redex where a hole is located in an evaluation context.
% So for example,
% \begin{center}
%   \begin{math}
%     \begin{array}{lll}
%       EXAMPLE
%     \end{array}
%   \end{math}
% \end{center}
% % section normal-order_reduction (end)


% \bibliographystyle{plain}
% \bibliography{thesis}
\end{document}
